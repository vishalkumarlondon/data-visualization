[
["index.html", "A Minimal Book Example Chapter 1 Prerequisites", " A Minimal Book Example Yihui Xie 2020-01-08 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 7. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["load-data.html", "Chapter 3 Load Data 3.1 Load packages 3.2 Download Airbnb data 3.3 Download Cultural Infrastructure data 3.4 Download London shapefile data 3.5 Join Airbnb data with LSOA shapefile 3.6 Join Culture data with LSOA shapefile 3.7 Join Airbnb, Culture and LSOA shapefile data 3.8 Download the LSOA profile data 3.9 Join LSOA profile data to the rest 3.10 Download Inner/Outer London boundaries 3.11 Clean variables before analysis", " Chapter 3 Load Data 3.1 Load packages #----load all the libraries needed library(sf) library(plyr) library(pins) library(tidyverse) library(tidyr) library(sp) #library(spdep) options(scipen = 999) 3.2 Download Airbnb data I collected every Airbnb “listings.csv” file for London from the website Inside Airbnb (http://insideairbnb.com/get-the-data.html) for the years 2017 and 2018, concatinated them all together and put it here (github link) on this project’s GitHub repo. In total, there are 242,490 unqiue listings with 14 columns of attributes. None of that raw data has had any post processing. #----read in Airbnb London listings data from the project GitHub repo as a dataframe airbnb &lt;- read_csv(&quot;https://raw.githubusercontent.com/vishalkumarlondon/CASA0005_coursework/master/data/airbnb-london-2017-2018.csv&quot;) print(dim(airbnb)) ## [1] 242490 14 This study does not look at the temporal change in price - i.e. the price difference between 2017 to 2018 - as the dependent variable. Rather, it averages out the price for each unique listing (by id) between 2017 and 2018. By doing this, adjustments for natural fluctations - for instance, seasonality - are averaged out. Future research may study dive deeper into the differences in price across time and season. #----create a new dataframe by grouping the listings id and creating an average price column price_average &lt;- airbnb %&gt;% group_by(id) %&gt;% summarise(price = mean(price)) colnames(price_average) &lt;- c(&quot;id&quot;, &quot;price_average&quot;) #----join the new dataframe to the original Airbnb data to add a column for the average price per listing between 2017 and 2018 airbnb &lt;- inner_join(airbnb, price_average, by = c(&#39;id&#39;)) #----keep the original Airbnb data my making a `airbnb_old` dataframe airbnb_old &lt;- airbnb #----remove unnecessary columns and remove duplicate listing ids to leave unique listings with average price between 2017 and 2018 #airbnb &lt;- airbnb[, ! colnames(airbnb) %in% c(&quot;price&quot;, &quot;last_review&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] airbnb &lt;- airbnb[, ! colnames(airbnb) %in% c(&quot;price&quot;)] airbnb &lt;- airbnb[!duplicated(airbnb[c(&quot;id&quot;)]),] #----turn Airbnb datafram into an sf airbnb &lt;- st_as_sf(airbnb, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) airbnb &lt;- st_transform(airbnb, 27700) At this stage, outliers in price are removed, this is because the average price for some listings went as high as £10,000. All listings that had an average price 2.58 times the mean were removed. Moreover, prices recorded as zero were also removed. #----calculate the mean, min and max and std of the Airbnb prices for the whole of London print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 45.0 80.0 100.8 125.0 9999.0 airbnb_price_mean &lt;- mean(airbnb$price_average) airbnb_price_std &lt;- sd(airbnb$price_average) airbnb_price_mad &lt;- mad(airbnb$price_average) #----then remove Airbnb where the price is two standard deviations away from the mean - i.e. outliers airbnb &lt;- subset(airbnb, price_average &lt; (airbnb_price_mean+(2.58*airbnb_price_std)) &amp; price_average &gt; (airbnb_price_mean-(2.58*airbnb_price_std))) #----then remove Airbnb where the price is zero airbnb &lt;- airbnb[airbnb$price_average != 0,] print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 45.00 79.00 93.08 120.00 397.00 3.3 Download Cultural Infrastructure data I collected data for every cultural venue in London by borough from data.london.gov (https://data.london.gov.uk/dataset/cultural-infrastructure-map). Specifically, the “Cultural venues by London borough” zip file was downloaded and then all the files were concatinated. In total there were 10,003 cultural venues in London. This entire dataset was then passed to the Google Places API to calculate the average user rating and total number of reviews for each venue. The total run time to recieve a ratings and reviews took 90 minutes, which is why it was much more efficient to calculated these metrics outside of this R script. For more information on the Google Place API, please visit this link (X) 1,958 venues out of the 10,003 total did not possess any, or had zero, user ratings and reviews on Google Places and were therefore dropped leaving 8,045 venues remaining for the analysis. #----read in the Cultural Infrastructure data from porject GitHubb culture &lt;- read.csv(&quot;https://raw.githubusercontent.com/vishalkumarlondon/CASA0005_coursework/master/data/all-cultural-infra-map-google-places.csv&quot;) print(dim(culture)) ## [1] 10003 25 print(colSums(is.na(culture))) ## BOROUGH Cultural.Venue.Type site_name address1 ## 0 0 0 0 ## address2 address3 borough_code borough_name ## 0 0 0 0 ## latitude longitude easting northing ## 0 0 0 0 ## os_addressbase_uprn ward_2018_code ward_2018_name website ## 4273 0 0 0 ## gss_code runtime API_response formatted_address ## 0 0 0 0 ## name place_id rating types ## 0 0 1556 0 ## user_ratings_total ## 1556 print(dim(culture[culture$rating == 0, ])) ## [1] 1958 25 #----only keep rows from Cultural Infrastructure if the longitude cell is filled in (i.e. not Null) culture &lt;- culture[complete.cases(culture$longitude), ] #----only keep rows from Cultural Infrastructure if the rating cell is filled in (i.e. not Null) culture &lt;- culture[!is.na(as.numeric(as.character(culture$rating))),] #----only keep rows from Cultural Infrastructure if the rating cell is not 0 culture &lt;- culture[culture$rating != 0, ] #----turn the Cultural Infrastructure dataframe into an sf object culture &lt;- st_as_sf(culture, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) culture &lt;- st_transform(culture, 27700) #culture_withPubs &lt;- culture #culture &lt;- culture[!culture$Cultural.Venue.Type == &#39;Pubs&#39;,] 3.4 Download London shapefile data The GIS shapefile boundaries for London were downloaded from data.london (https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london). This research is interested in studying the Lower Super Output Area (LSOA) 2011 boundary area, however, Ward and Borough level shape files were also stored in a list if necessary. In total there are 4,835 LSOA areas. #----the following code has been adapted from (MacLachlan &amp; Dennett, 2019: Section 10.4.1) #----use the pin function from the pins package to store the GIS London boundary .zip files from data.london pin_london_GIS &lt;- pin(&quot;https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip&quot;) #----grab the shape files for Borough, Ward and LSOA based on their string values and cache s &lt;- grepl(&quot;Borough|Ward_|LSOA_2011&quot;, pin_london_GIS) &amp; grepl(&quot;.shp$&quot;, pin_london_GIS) #----create a list for Borough, Ward and LSOA shape files BoroughsWardsLSOA &lt;- pin_london_GIS[s] #----turn each element in the list into a SF file using the st_read function BoroughsWardsLSOAsf &lt;- lapply(BoroughsWardsLSOA, st_read) ## Reading layer `London_Borough_Excluding_MHW&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 33 features and 7 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Reading layer `London_Ward_CityMerged&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 625 features and 7 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Reading layer `LSOA_2011_London_gen_MHW&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4835 features and 14 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs BoroughsWardsLSOAsf &lt;- lapply(BoroughsWardsLSOAsf, crs=27700, st_transform) #----create a variable for LSOAs in London by selecting the third element in the list londonLSOA &lt;- BoroughsWardsLSOAsf[[3]] 3.5 Join Airbnb data with LSOA shapefile The functions below join the Airbnb data to the LSOA areas in London and a) count the number of Airbnb listings per LSOA, b) calculating the average price of Airbnb lisitngs per LSOA, c) counting the number of Airbnb user reviews per LSOA, and d) calculating the average number of Airbnb user reviews per LSOA. JoinAirbnb_count &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----count the number of points per LSOA count &lt;- as.data.frame(plyr::count(joined$LSOA11CD)) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_freq&quot;) return(count) } JoinAirbnb_price &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA price &lt;- aggregate(price_average~LSOA11CD, joined, mean) names(price) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_price&quot;) return(price) } JoinAirbnb_NOreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA reviews &lt;- aggregate(number_of_reviews~LSOA11CD, joined, sum) names(reviews) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_no_reviews&quot;) return(reviews) } JoinAirbnb_AVreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA reviews &lt;- aggregate(number_of_reviews~LSOA11CD, joined, mean) names(reviews) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_av_reviews&quot;) return(reviews) } #----//////////////////////////////////////////////////////////////////////// #----AIRBNB COUNT----# #----use the first function to count the number of Airbnbs in each LSOA airbnbLSOA_count &lt;- JoinAirbnb_count(airbnb, londonLSOA) #----AIRBNB PRICE----# #----use the second function to calculate the average price of Airbnb listings in each LSOA airbnbLSOA_price &lt;- JoinAirbnb_price(airbnb, londonLSOA) #----AIRBNB NUMBER OF REVIEWS----# #----use the third function to count the number of Airbnb reviews in each LSOA airbnbLSOA_review_count &lt;- JoinAirbnb_NOreviews(airbnb, londonLSOA) #----AIRBNB AVERAGE NUMBER OF REVIEWS----# #----use the fourth function to calculate the average number of Airbnb reviews in each LSOA airbnbLSOA_review_average &lt;- JoinAirbnb_AVreviews(airbnb, londonLSOA) 3.6 Join Culture data with LSOA shapefile The functions below join the Cultural Infrastructure data to the LSOA areas in London and a) count the number of Cultural Infrastructure by cultural venue type per LSOA, b) calculating the average Google Places user rating by cultural venue type per LSOA, c) calculating the average number of Google Places user reviews by cultural venue type per LSOA, and d) counting the total number of Google Places user reviews by cultural venue type per LSOA. JoinCulture_count &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----count the number cultural venue types per LSOA #source----https://stackoverflow.com/questions/10879551/frequency-count-of-two-column-in-r count &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), nrow) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_freq&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr count &lt;- count %&gt;% spread(cultural_venue_type, culture_freq) #----then use the rowSums function to sum up the counts from all cultural venue type, skip NA values and create new column count$culture_freq &lt;- rowSums(count[,sapply(count, is.numeric)], na.rm=TRUE) return(count) } JoinCulture_rating &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average rating of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group average &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) mean(x$rating)) names(average) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_rating&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr average &lt;- average %&gt;% spread(cultural_venue_type, culture_rating) #----then use the rowSums function to average the ratings from all cultural venue type, skip NA values and create new column average$culture_rating &lt;- rowMeans(average[,sapply(average, is.numeric)], na.rm=TRUE) return(average) } JoinCulture_NOreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the sum of reviews of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group count &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) sum(x$user_ratings_total)) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_no_reviews&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr count &lt;- count %&gt;% spread(cultural_venue_type, culture_no_reviews) #----then use the rowSums function to sum up the reviews from all cultural venue type, skip NA values and create new column count$culture_no_reviews &lt;- rowSums(count[,sapply(count, is.numeric)], na.rm=TRUE) return(count) } JoinCulture_AVreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average rating of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group average &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) mean(x$user_ratings_total)) names(average) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_av_reviews&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr average &lt;- average %&gt;% spread(cultural_venue_type, culture_av_reviews) #----then use the rowSums function to average the reviews from all cultural venue type, skip NA values and create new column average$culture_av_reviews &lt;- rowMeans(average[,sapply(average, is.numeric)], na.rm=TRUE) return(average) } #----//////////////////////////////////////////////////////////////////////// #----CULTURE COUNT----# #----use the first function to count the number for each cultural venue category in each LSOA cultureLSOA_count &lt;- JoinCulture_count(culture, londonLSOA) #----CULTURE RATING----# #----use the second function to calculate the average rating of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_rating &lt;- JoinCulture_rating(culture, londonLSOA) #----CULTURE NUMBER OF REVIEWS----# #----use the third function to count the number of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_review_count &lt;- JoinCulture_NOreviews(culture, londonLSOA) #----CULTURE AVERAGE NUMBER OF REVIEWS----# #----use the third function to count the number of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_review_average &lt;- JoinCulture_AVreviews(culture, londonLSOA) 3.7 Join Airbnb, Culture and LSOA shapefile data Having done the previous calculations for Airbnb and Cultural Infrastructure per LSOA, all the data is joined together. #----merge all dataframes into one #https://stackoverflow.com/questions/8091303/simultaneously-merge-multiple-data-frames-in-a-list #install safejoin package from GitHub devtools::install_github(&quot;moodymudskipper/safejoin&quot;) library(safejoin) #----use eat function from safejoin to merge a list of all the dataframes londonLSOAextradata &lt;- eat(airbnbLSOA_count, list(airbnbLSOA_price, airbnbLSOA_review_count, airbnbLSOA_review_average, cultureLSOA_count, cultureLSOA_rating, cultureLSOA_review_count, cultureLSOA_review_average), .by = &quot;LSOA11CD&quot;, .conflict = ~.x) 3.8 Download the LSOA profile data The LSOA atlas (https://data.london.gov.uk/dataset/lsoa-atlas) provides a summary of demographic and related data for each Lower Super Output Area in Greater London. Some of this attribbute data for each LSOA will be useful as independent variable later in the analysis. This data is downloaded from data.london and subset by 15 columns which are most relevant as per Dudas et al’s (2017) paper. #----run the code below if you want to read in LSOA attribute data using the current 2011 boundaries #----NOTE: There is comparatively less data for the new boundaries compared with the old boundaries londonLSOAProfiles &lt;- read_csv(&quot;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&quot;, na = c(&quot;&quot;, &quot;NA&quot;, &quot;n/a&quot;), locale = locale(encoding = &#39;Latin1&#39;), col_names = TRUE) ## Warning: 2 parsing failures. ## row col expected actual file ## 1348 House Prices;Median Price (£);2014 a double . &#39;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&#39; ## 2873 House Prices;Median Price (£);2014 a double . &#39;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&#39; select.me &lt;- c(&#39;Lower Super Output Area&#39;, &#39;Population Density;Area (Hectares);&#39;, &#39;Population Density;Persons per hectare;2013&#39;, &#39;Ethnic Group;BAME (%);2011&#39;, &#39;Country of Birth;% Not United Kingdom;2011&#39;, &#39;Tenure;Owned outright (%);2011&#39;, &#39;Tenure;Owned with a mortgage or loan (%);2011&#39;, &#39;House Prices;Median Price (£);2014&#39;, &#39;Economic Activity;Employment Rate;2011&#39;, &#39;Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011&#39;, &#39;Household Income, 2011/12;Median Annual Household Income estimate (£)&#39;, &#39;Public Transport Accessibility Levels (2014);% 4-6 (good access)&#39;, &#39;2013 Census Population;Age Structure;16-29&#39;, &#39;2014 Census Population;Age Structure;30-44&#39;, &#39;Dwelling type;All Households;2011&#39;) londonLSOAProfiles &lt;- londonLSOAProfiles[,select.me] 3.9 Join LSOA profile data to the rest Now that we have all of our data, we join them all together to create the londonLSOAProfiles sf object. Moreover, we caclulate the frequency density of cultural venue type per km^2 in every LSAO as per #----merge the LSOA boundaries shapefile with the and LSOA attribute dataframe londonLSOAProfiles &lt;- inner_join(londonLSOA, londonLSOAProfiles, by = c(&quot;LSOA11CD&quot; = &quot;Lower Super Output Area&quot;)) ## Warning: Column `LSOA11CD`/`Lower Super Output Area` joining factor and ## character vector, coercing into character vector #londonLSOAProfiles &lt;- na.omit(londonLSOAProfiles) #----join the extra data - Airbnb price &amp; counts and culture counts - to the LSAO profile data londonLSOAProfiles &lt;- inner_join(londonLSOAProfiles, londonLSOAextradata, by = c(&#39;LSOA11CD&#39;)) ## Warning: Column `LSOA11CD` joining character vector and factor, coercing into ## character vector 3.10 Download Inner/Outer London boundaries It will be useful to compare Airbnb lisitings and cultural infrastructure in Inner London vs Outer London as it is likely that the freqncy, price and average ratings of the former will all be higher than the later. Download the Inner/Outer London boundaries from data.london and join with our dataset. #----use the pin function from the pins package to store the Inner and Outer London boundary .zip files from data.london pin_inner_outer &lt;- pin(&quot;https://data.london.gov.uk/download/inner-and-outer-london-boundaries-london-plan-consultation-2009/684e59f2-9208-4da1-bf67-d8dfeb72c047/lp-consultation-oct-2009-inner-outer-london-shp.zip&quot;) #----pull out the Inner and Outer London shapefile i_o &lt;- grepl(&quot;lp-consultation-oct-2009-inner-outer&quot;, pin_inner_outer) &amp; grepl(&quot;.shp$&quot;, pin_inner_outer) #----create a list for the shape file inner_outer &lt;- pin_inner_outer[i_o] #----turn the shapefile into a SF file using the st_read function inner_outerSF &lt;- st_read(inner_outer) ## Reading layer `lp-consultation-oct-2009-inner-outer-london&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/lp_consultation_oct_2009_inner_outer_london_shp/lp-consultation-oct-2009-inner-outer-london.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 2 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs st_transform(inner_outerSF, 27700) ## Simple feature collection with 2 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Boundary Source Area_Ha Shape_Leng Shape_Area ## 1 Inner London London Plan Consultation Draft 34863.3 117020.7 348632957 ## 2 Outer London London Plan Consultation Draft 124606.8 373367.7 1246068121 ## geometry ## 1 POLYGON ((522055.6 178014.7... ## 2 POLYGON ((503611.2 175520.4... #----join the shapefile to the LSOA SF obbject on geometry inner_outer_df &lt;- st_join(londonLSOA, inner_outerSF, by = c(&quot;geometry&quot; = &quot;geometry&quot;)) inner_outer_df &lt;- as.data.frame(inner_outer_df) names(inner_outer_df)[names(inner_outer_df) == &#39;Boundary&#39;] &lt;- &#39;InnerOuter&#39; select.me &lt;- c(&#39;LSOA11CD&#39;,&#39;InnerOuter&#39;) inner_outer_df &lt;- inner_outer_df[,select.me] #----join the Inner and Outer London extra data to the LSAO profile data londonLSOAProfiles &lt;- inner_join(londonLSOAProfiles, inner_outer_df, by = c(&quot;LSOA11CD&quot; = &quot;LSOA11CD&quot;)) ## Warning: Column `LSOA11CD` joining character vector and factor, coercing into ## character vector #----drop duplicate rows for LSOA11CD, culture_freq, airbnb_price, airbnb_freq columns londonLSOAProfiles &lt;- londonLSOAProfiles[!duplicated(londonLSOAProfiles[c(&quot;LSOA11CD&quot;, &quot;culture_freq&quot;, &quot;airbnb_price&quot;, &quot;airbnb_freq&quot;)]),] #londonLSOAProfiles &lt;- londonLSOAProfiles[londonLSOAProfiles$InnerOuter == &#39;Inner London&#39;,] 3.11 Clean variables before analysis Some variable names are changed before the analysis for ease of use and also to align with Dudas et al (2017). #----change the column names of some of the independent variables colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Population Density;Area (Hectares);&quot;)] &lt;- &quot;areaLSOA&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Population Density;Persons per hectare;2013&quot;)] &lt;- &quot;pop_density&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Ethnic Group;BAME (%);2011&quot;)] &lt;- &quot;bame_p&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Country of Birth;% Not United Kingdom;2011&quot;)] &lt;- &quot;nonUK&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Tenure;Owned outright (%);2011&quot;)] &lt;- &quot;house_own&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Tenure;Owned with a mortgage or loan (%);2011&quot;)] &lt;- &quot;house_mortg&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;House Prices;Median Price (£);2014&quot;)] &lt;- &quot;house_price&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Economic Activity;Employment Rate;2011&quot;)] &lt;- &quot;employees&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011&quot;)] &lt;- &quot;education&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Household Income, 2011/12;Median Annual Household Income estimate (£)&quot;)] &lt;- &quot;income&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Public Transport Accessibility Levels (2014);% 4-6 (good access)&quot;)] &lt;- &quot;transport&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;2013 Census Population;Age Structure;16-29&quot;)] &lt;- &quot;age16_29&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;2014 Census Population;Age Structure;30-44&quot;)] &lt;- &quot;age30_44&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Dwelling type;All Households;2011&quot;)] &lt;- &quot;housing&quot; #----change the some of the independent variables to density values by dividing by the area of the LSOA in hectares - areaLSOA londonLSOAProfiles$young_p &lt;- ((londonLSOAProfiles$age16_29 + londonLSOAProfiles$age30_44)/londonLSOAProfiles$areaLSOA)*100 londonLSOAProfiles$housing &lt;- (londonLSOAProfiles$housing/londonLSOAProfiles$areaLSOA)*100 #----calculate the km^2 frequency density of Airbnb per LSAO (as per Dudas et al, 2017) londonLSOAProfiles$airbnb_freq &lt;- (londonLSOAProfiles$airbnb_freq/londonLSOAProfiles$areaLSOA)*100 londonLSOAProfiles$airbnb_no_reviews &lt;- (londonLSOAProfiles$airbnb_no_reviews/londonLSOAProfiles$areaLSOA)*100 #----find the column numbers for first and last cultural venue type which( colnames(londonLSOAProfiles)==&quot;Archives&quot; ) ## [1] 33 which( colnames(londonLSOAProfiles)==&quot;culture_freq&quot; ) ## [1] 66 #----function to change variable from freqency to freqency density per km2 by dividing by the LSOA hectare size and muliplying by 100 A &lt;- function(x) (x / londonLSOAProfiles$`areaLSOA`)*100 #----calculate the km^2 frequency density of cultural venue type per LSAO (as per Dudas et al, 2017) londonLSOAProfiles[33:66] &lt;- lapply(londonLSOAProfiles[33:66], A) ## Warning in `[&lt;-.data.frame`(`*tmp*`, 33:66, value = list(Archives = ## c(23.0769230769231, : provided 35 variables to replace 34 variables # # #devtools::install_github(&#39;rstudio/bookdown&#39;) # library(bookdown) # preview_chapter(&quot;02-load-data.Rmd&quot;) "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Airbnb EDA 4.2 Cultural infrastructure EDA", " Chapter 4 Exploratory Data Analysis Having retrieved all the necessary data, we now perform some exploratory data analysis on the variables. Before doing the EDA, we load in some very useful data visualization libraries used by this book Fundementals of Data Visualization - https://serialmentor.com/dataviz/geospatial-data.html #----load all the libraries needed # load in libraries library(tidyverse) library(scales) library(lubridate) library(ggridges) library(gridExtra) #----data visualization packages - https://serialmentor.com/dataviz/geospatial-data.html #install.packages(&quot;remotes&quot;) #install.packages(&quot;devtools&quot;) library(remotes) #install.packages(&quot;cowplot&quot;) #devtools::install_github(&quot;wilkelab/cowplot&quot;) library(cowplot) #install.packages(&quot;colorspace&quot;) library(colorspace) #devtools::install_github(&quot;clauswilke/colorblindr&quot;) #https://rdrr.io/github/clauswilke/dviz.supp/ #devtools::install_github(&quot;clauswilke/dviz.supp&quot;) library(dviz.supp) options(scipen = 999) Then, we set up some basic settings from this great Kaggle Kernel by X for data visualisation of exploratory data analysis of variables https://www.kaggle.com/jaseziv83/a-deep-dive-eda-into-all-variables/report #----set the plotting theme baseline theme_set(theme_minimal() + theme(axis.title.x = element_text(size = 15, hjust = 1), axis.title.y = element_text(size = 15), axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), panel.grid.major = element_line(linetype = 2), panel.grid.minor = element_line(linetype = 2), plot.margin=unit(c(1,1,-0.5,1),&quot;cm&quot;), plot.title = element_text(size = 18, colour = &quot;grey25&quot;, face = &quot;bold&quot;), plot.subtitle = element_text(size = 16, colour = &quot;grey44&quot;))) col_pal &lt;- c(&quot;#5EB296&quot;, &quot;#4E9EBA&quot;, &quot;#F29239&quot;, &quot;#C2CE46&quot;, &quot;#FF7A7F&quot;, &quot;#4D4D4D&quot;) 4.1 Airbnb EDA First we do EDA on the Airbnb data 4.1.1 Airbnb freqency distributions When looking at the distribution of Airbnb listings we see that there are some outliter. After #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_freq), 0) plot1 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_freq &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_freq)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB FREQUENCY&quot;, subtitle = &quot;Airbnb freqency still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb freq per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_price), 0) plot2 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_price &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_price)) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB PRICE&quot;, subtitle = &quot;Airbnb price still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb price per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_av_reviews), 0) plot3 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_av_reviews &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_av_reviews)) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB REVIEWS&quot;, subtitle = &quot;Airbnb reviews still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb reviews per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) g &lt;- grid.arrange(plot1, plot2, plot3, ncol=3) #ggsave(&quot;graphs/1.png&quot;, plot = g, width = 10, height = 4) 4.1.2 Airbnb log freqency distributions Lets look at the log of Airbnb freqency #----this log distribution function was taken from Zivkovic (2019) plot4 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_freq))) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB FREQ&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot5 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_price))) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB PRICE&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb price)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot6 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_no_reviews))) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB REVIEWS&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb reviews)&quot;, y= &quot;Count&quot;) grid.arrange(plot4, plot5, plot6, ncol=3) 4.1.3 Airbnb Inner vs Outer London Lets look at Airbnb frequency in Inner vs Outer London #----this log distribution function was taken from Zivkovic (2019) plot7 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_freq), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;THERE ARE MORE LISTINGS IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) #----this log distribution function was taken from Zivkovic (2019) plot8 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_price), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;LISTINGS ARE MORE EXPENSIVE IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Price)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) #----this log distribution function was taken from Zivkovic (2019) plot9 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_no_reviews), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;LISTINGS ARE MORE EXPENSIVE IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Price)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) grid.arrange(plot7, plot8, plot9, ncol=3) 4.2 Cultural infrastructure EDA Explaination. 4.2.1 Cultural infrastructure venues #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$culture_freq, na.rm = TRUE), 0) plot10 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(culture_freq &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=culture_freq)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE IN FREQUENCY&quot;, subtitle = &quot;Cultural Infrastructure freqency still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure freq per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$culture_rating, na.rm = TRUE), 0) plot11 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(culture_rating &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=culture_rating)) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE PRICE&quot;, subtitle = &quot;Cultural Infrastructure price still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure price per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_av_reviews, na.rm = TRUE), 0) plot12 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_av_reviews &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_av_reviews)) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE REVIEWS&quot;, subtitle = &quot;Cultural Infrastructure reviews still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure reviews per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) grid.arrange(plot10, plot11, plot12, ncol=3) ## Warning: Removed 2117 rows containing non-finite values (stat_bin). ## Warning: Removed 2117 rows containing non-finite values (stat_bin). 4.2.2 Cultural infrastructure ratings gather_count &lt;- londonLSOAProfiles %&gt;% gather(culture, culture_density, Archives:Theatres) #gather_ratings &lt;- londonLSOAProfiles %&gt;% gather(culture, culture_rating, Archives:Theatres) 4.2.3 Cultural infrastructure ratings Correlations # ggplot(londonLSOAProfiles, aes(log(airbnb_freq), log(airbnb_price))) + # geom_point(pch = 21, fill = &quot;gray25&quot;, color = &quot;white&quot;, size = 2.5) + # scale_x_continuous(name = &quot;airbnb freq per LSOA (km2)&quot;) + # scale_y_continuous(name = &quot;airbnb price per LSOA (km2)&quot;) + # theme_dviz_grid() ggplot(londonLSOAProfiles, aes(log(airbnb_freq), log(airbnb_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;airbnb freq per LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb price per LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( #legend.position = c(1, 0.01), #legend.justification = c(1, 0), legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ggplot(londonLSOAProfiles, aes(log(airbnb_freq), log(airbnb_no_reviews), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;airbnb freq per LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb reviews per LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( #legend.position = c(1, 0.01), #legend.justification = c(1, 0), legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) "],
["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words Now we are going to runa regression on #----load all the libraries needed # load in libraries library(tidyverse) library(scales) library(lubridate) library(ggridges) library(gridExtra) #regression library(corrplot) library(rgdal) library(spdep) library(car) #----data visualization packages - https://serialmentor.com/dataviz/geospatial-data.html #install.packages(&quot;remotes&quot;) #install.packages(&quot;devtools&quot;) library(remotes) #install.packages(&quot;cowplot&quot;) #devtools::install_github(&quot;wilkelab/cowplot&quot;) library(cowplot) #install.packages(&quot;colorspace&quot;) library(colorspace) #devtools::install_github(&quot;clauswilke/colorblindr&quot;) #https://rdrr.io/github/clauswilke/dviz.supp/ #devtools::install_github(&quot;clauswilke/dviz.supp&quot;) library(dviz.supp) options(scipen = 999) #run a final OLS model model_freq &lt;- lm(log(`airbnb_freq`) ~ `bame_p` + `nonUK` + `education` + `income` + `house_mortg` + `house_price` + `culture_freq` + `culture_rating`+ `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) summary(model_freq) ## ## Call: ## lm(formula = log(airbnb_freq) ~ bame_p + nonUK + education + ## income + house_mortg + house_price + culture_freq + culture_rating + ## InnerOuter, data = londonLSOAProfiles, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3974 -0.5181 0.0567 0.6118 2.4763 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.632e+00 2.693e-01 13.484 &lt; 2e-16 *** ## bame_p -1.060e-02 2.275e-03 -4.661 3.34e-06 *** ## nonUK 2.300e-02 3.030e-03 7.590 4.70e-14 *** ## education 6.484e-02 2.816e-03 23.021 &lt; 2e-16 *** ## income -5.983e-05 4.071e-06 -14.695 &lt; 2e-16 *** ## house_mortg -3.254e-02 3.042e-03 -10.698 &lt; 2e-16 *** ## house_price 3.663e-07 8.408e-08 4.356 1.38e-05 *** ## culture_freq 8.879e-03 7.881e-04 11.266 &lt; 2e-16 *** ## culture_rating 1.259e-01 5.309e-02 2.371 0.0178 * ## InnerOuterOuter London -1.345e+00 5.373e-02 -25.038 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9203 on 2204 degrees of freedom ## (2119 observations deleted due to missingness) ## Multiple R-squared: 0.7398, Adjusted R-squared: 0.7388 ## F-statistic: 696.4 on 9 and 2204 DF, p-value: &lt; 2.2e-16 #and for future use, write the residuals out to a column in your dataframe londonLSOAProfiles$model_freq_resids &lt;- residuals(model_freq) # #https://stackoverflow.com/questions/6882709/how-do-i-deal-with-nas-in-residuals-in-a-regression-in-r # #how to deal with residuals with NA - make use of the row names associated with the data frame provided as input to lm # #londonLSOAProfiles[names(model_price$model_price_resids), &quot;residual&quot;] &lt; - model_price$residuals # # #use the boom library for residuals - https://stackoverflow.com/questions/17216358/eliminating-nas-from-a-ggplot # library(broom) # df &lt;- augment(model_freq) londonLSOAProfiles %&gt;% drop_na(model_freq_resids) %&gt;% ggplot(aes(x= model_freq_resids)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;RESIDUAL DISTRUBUTION&quot;, subtitle = &quot;Residual distrubution of the Airbnb Price model&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) vif(model_freq) ## bame_p nonUK education income house_mortg ## 4.535196 3.985935 4.447229 6.350560 2.833953 ## house_price culture_freq culture_rating InnerOuter ## 2.762761 1.285302 1.028660 1.859964 plot(model_freq) #nona &lt;- londonLSOAProfiles[londonLSOAProfiles$model_freq_resids != 0] #run durbin-watson test durbinWatsonTest(model_freq$residuals) ## [1] 1.397353 library(sf) library(tmap) tm_shape(londonLSOAProfiles) + tm_polygons(&quot;model_freq_resids&quot;, palette = &quot;RdYlBu&quot;) ## Warning: The shape londonLSOAProfiles is invalid. See sf::st_is_valid "],
["references.html", "References", " References "],
["literature.html", "Chapter 6 Literature", " Chapter 6 Literature Here is a review of existing methods. "],
["methods.html", "Chapter 7 Methods", " Chapter 7 Methods We describe our methods in this chapter. "],
["applications.html", "Chapter 8 Applications 8.1 Example one 8.2 Example two", " Chapter 8 Applications Some significant applications are demonstrated in this chapter. 8.1 Example one 8.2 Example two "],
["final-words-1.html", "Chapter 9 Final Words", " Chapter 9 Final Words We have finished a nice book. "]
]
